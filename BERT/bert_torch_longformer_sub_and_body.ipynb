{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahzYEgevkwH4",
        "outputId": "e33bccd2-c631-4668-dc6b-e3ddc0e12ab3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import io\n",
        "import pandas as pd\n",
        "from transformers import LongformerTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import LongformerForSequenceClassification, AdamW, BertConfig, get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tabulate import tabulate\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tygnHWslhWV",
        "outputId": "4c0035b0-07f8-4ea6-fb71-15e478d9ffd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP1-lpB0pQKG",
        "outputId": "8c401570-a607-4d0c-f12d-de9b14aecb87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11448, 3)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"drive/MyDrive/preprocessed_sub_and_body.csv\")\n",
        "df[\"text\"] = df[\"text\"].astype(str)\n",
        "df[\"label\"] = df[\"label\"].astype(int)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1Zpv9BfD7DL"
      },
      "outputs": [],
      "source": [
        "average_length = df['length'].mean()\n",
        "max_length = df['length'].max()\n",
        "min_length = df['length'].min()\n",
        "\n",
        "print(f\"Average length: {average_length}\")\n",
        "print(f\"Max length: {max_length}\")\n",
        "print(f\"Min length: {min_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enrdNrq9A2Ye"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(df['text'],df['label'],\n",
        "                                                    stratify=df['label'],\n",
        "                                                    test_size=0.2)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, random_state=42, test_size=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vpnt6zA7KBCz"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAY_wObTy5hq"
      },
      "outputs": [],
      "source": [
        "def encode_data(input_text):\n",
        "    tokens = tokenizer.batch_encode_plus(\n",
        "        input_text,\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        max_length=1536,\n",
        "        padding=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    return tokens[\"input_ids\"], tokens[\"attention_mask\"]\n",
        "\n",
        "\n",
        "train_input_ids, train_attention_masks = encode_data(list(X_train))\n",
        "val_input_ids, val_attention_masks = encode_data(list(X_val))\n",
        "\n",
        "# Convert labels to PyTorch tensors\n",
        "train_labels = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "val_labels = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca6gBl6mf1Pd"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "num_epochs = 4\n",
        "lr = 3e-5\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n",
        "validation_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOFKVLingEaK"
      },
      "outputs": [],
      "source": [
        "bert_model = LongformerForSequenceClassification.from_pretrained(\n",
        "    \"allenai/longformer-base-4096\",\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "\n",
        "optimizer = AdamW(bert_model.parameters(), lr=lr)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 4\n",
        "\n",
        "\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzsuW-cBouHi"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "bert_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP85fQLYpfTd"
      },
      "outputs": [],
      "source": [
        "progress_bar = tqdm(range(num_training_steps))  # proress bar\n",
        "\n",
        "# initialize lists to store metrics\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "val_precisions = []\n",
        "val_recalls = []\n",
        "val_f1_scores = []\n",
        "val_roc_aucs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\" Epoch {:} / {:}\".format(epoch + 1, num_epochs))\n",
        "    # reset the total loss for each epoch\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # put the model into training mode\n",
        "    bert_model.train()\n",
        "\n",
        "    # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        (b_input_ids, b_input_mask, b_labels) = tuple(\n",
        "            t.to(device) for t in batch\n",
        "        )  # send tesors to gpu\n",
        "\n",
        "        # ensure labels are of type LongTensor\n",
        "        b_labels = b_labels.long()\n",
        "\n",
        "        # setting the gradients to zero, cuz PyTorch doesn't do this automatically\n",
        "        # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        outputs = bert_model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "        pred = outputs.logits\n",
        "        loss = loss_fn(pred, b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()  # accumulate loss over all batches\n",
        "\n",
        "        # backward pass/calculating gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # optimizing the parameters of the bert model using computed gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # update progress bar\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # calculate the average loss\n",
        "    epoch_train_loss = total_train_loss / len(train_loader)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    print(f\"\\nTraining epoch {epoch + 1} loss: \", epoch_train_loss)\n",
        "\n",
        "    # TESTING BLOCK STARTS\n",
        "    total_val_loss = 0\n",
        "\n",
        "    # put the model in evaluation mode\n",
        "    bert_model.eval()\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    for i, batch in enumerate(validation_loader):\n",
        "        (b_input_ids, b_input_mask, b_labels) = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # ensure labels are of type LongTensor\n",
        "        b_labels = b_labels.long()\n",
        "\n",
        "        # no gradients needed for testning/validation\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "\n",
        "        total_val_loss += loss.item()\n",
        "\n",
        "        # get predictions\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        all_val_preds.extend(preds.cpu().numpy())\n",
        "        all_val_labels.extend(b_labels.cpu().numpy())\n",
        "\n",
        "    epoch_val_loss = total_val_loss / len(validation_loader)\n",
        "    accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
        "    precision = precision_score(all_val_labels, all_val_preds, average=\"binary\")\n",
        "    recall = recall_score(all_val_labels, all_val_preds, average=\"binary\")\n",
        "    f1 = f1_score(all_val_labels, all_val_preds, average=\"binary\")\n",
        "    roc_auc = roc_auc_score(all_val_labels, all_val_preds)\n",
        "\n",
        "    print(f\"\\nValidation epoch {epoch + 1} loss: {epoch_val_loss}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "    print(f\"ROC-AUC: {roc_auc}\")\n",
        "\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracies.append(accuracy)\n",
        "    val_precisions.append(precision)\n",
        "    val_recalls.append(recall)\n",
        "    val_f1_scores.append(f1)\n",
        "    val_roc_aucs.append(roc_auc)\n",
        "\n",
        "    #  early stopping\n",
        "    # if avg_val_loss < best_val_loss or f1 > best_f1:\n",
        "    #     best_val_loss = avg_val_loss\n",
        "    #     best_f1 = f1\n",
        "    #     trigger_times = 0\n",
        "    #     # Save the best model weights\n",
        "    #     torch.save(bert_model.state_dict(), 'best_model.pt')\n",
        "    # else:\n",
        "    #     trigger_times += 1\n",
        "    #     if trigger_times >= patience:\n",
        "    #         print(\"Early stopping!\")\n",
        "    #         bert_model.load_state_dict(torch.load('best_model.pt'))\n",
        "    #         break\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efN0V73TpspC"
      },
      "outputs": [],
      "source": [
        "model_save_path = \"drive/MyDrive/bert_lonformer_model.pt\"\n",
        "optimizer_save_path = \"drive/MyDrive/longformer_optimizer.pt\"\n",
        "\n",
        "torch.save(bert_model.state_dict(), model_save_path)\n",
        "\n",
        "torch.save(optimizer.state_dict(), optimizer_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-Wq7qWa0YD2"
      },
      "outputs": [],
      "source": [
        "# !pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the metrics in tabular format\n",
        "table_data = [\n",
        "    [\n",
        "        \"Epoch\",\n",
        "        \"Train Loss\",\n",
        "        \"Val Loss\",\n",
        "        \"Val Acc\",\n",
        "        \"Val Precision\",\n",
        "        \"Val Recall\",\n",
        "        \"Val F1\",\n",
        "        \"Val ROC-AUC\",\n",
        "    ],\n",
        "]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    table_data.append(\n",
        "        [\n",
        "            epoch + 1,\n",
        "            train_losses[epoch],\n",
        "            val_losses[epoch],\n",
        "            val_accuracies[epoch],\n",
        "            val_precisions[epoch],\n",
        "            val_recalls[epoch],\n",
        "            val_f1_scores[epoch],\n",
        "            val_roc_aucs[epoch],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "print(tabulate(table_data, headers=\"firstrow\", floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaNMKNQ8atRK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMhnQesjud89"
      },
      "source": [
        "# Evaluacija nad testnim podacima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G4X4tlXudqq"
      },
      "outputs": [],
      "source": [
        "test_input_ids, test_attention_masks = encode_data(list(X_test))\n",
        "\n",
        "test_labels = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        (b_input_ids, b_input_mask, b_labels) = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        b_labels = b_labels.long()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "        # get predictions\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(b_labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_test_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, f1, roc_auc\n",
        "\n",
        "\n",
        "test_loss, test_accuracy, test_precision, test_recall, test_f1, test_roc_auc = (\n",
        "    evaluate_model(bert_model, test_loader, device)\n",
        ")\n",
        "\n",
        "print(f\"Test loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Test ROC-AUC: {test_roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_Cmc1U8suuj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
